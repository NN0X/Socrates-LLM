#include <iostream>

// TODO: create a subword tokenizer
// TODO: use "Attention is All You Need" in GPT architecture
// TODO: use Adam optimizer to train the model (gradient descent)
